{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yogee11/A2A/blob/main/Chaos_Theory_Based_Compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Created By Ash Kelly**\n",
        "\n",
        "**Notebook Description: Chaos Codec - Time-Series Compression using Predictive Modeling**\n",
        "\n",
        "This notebook implements and evaluates a custom, lossy compression algorithm, termed \"Chaos Codec,\" designed specifically for chaotic time-series data. It uses the logistic map as a representative chaotic signal and leverages predictive modeling techniques to achieve compression. The performance (both compression ratio and reconstruction accuracy) is compared against standard lossless compression algorithms.\n",
        "\n",
        "**Methodology:**\n",
        "\n",
        "1.  **Signal Generation:** A large chaotic time series (~10MB of float64 values) is generated using the logistic map equation (`r=4.0`, ensuring chaotic behavior).\n",
        "2.  **Base Predictive Model:** A linear regression model, inspired by the structure of the HÃ©non map, is trained to predict the next value in the series based on the previous two values and the square of the immediate predecessor (`x[t+1] ~ W1*x[t-1] + W2*x[t] + W3*x[t]^2 + b`).\n",
        "3.  **Base Signal Reconstruction:** The trained base model is used autoregressively to generate an approximation of the original signal.\n",
        "4.  **Residual Modeling (Mirror-Based):** The residuals (errors between the original signal and the base reconstruction) are calculated. A *second* linear regression model is trained to predict these residuals. Uniquely, this model uses features derived from the *reversed* (mirrored) original signal (`residual[t] ~ R1*original_reversed[t] + R2*original_reversed[t+1] + Rb`).\n",
        "5.  **Hybrid Reconstruction:** The predictions from the residual model are added to the base reconstruction to create a more accurate \"hybrid\" signal.\n",
        "6.  **Delta Correction & Quantization:** The final difference (delta) between the original signal and the hybrid reconstruction is computed. This delta is then quantized to `int16` after scaling (introducing the lossy aspect of the codec), pickled, and compressed using `zlib`. This forms the main \"residual payload.\"\n",
        "7.  **Final Reconstruction:** The signal is reconstructed one last time by adding the de-quantized delta back to the hybrid signal.\n",
        "8.  **Model Parameter Compression:** The coefficients and intercepts from both linear models are collected, converted to `float16` for space saving, pickled, and compressed using `zlib`.\n",
        "9.  **Evaluation:** The accuracy of the final reconstructed signal is measured against the original using Mean Squared Error (MSE), Mean Absolute Error (MAE), Cosine Similarity, and the Loss Ratio (MSE / Signal Variance).\n",
        "10. **Benchmarking:** The original signal is compressed using standard lossless algorithms (`zlib`, `gzip`, `bz2`, `lzma`) to establish baseline compression ratios.\n",
        "11. **Comparison:** The total size of the \"Chaos Codec\" (compressed model parameters + compressed residual payload) is compared against the sizes achieved by the standard compressors. Compression ratios relative to the original pickled data size are calculated and reported alongside the accuracy metrics.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "The notebook aims to explore the feasibility of using chained predictive models tailored to the dynamics of a chaotic system as a form of lossy compression. It quantifies the trade-off between the compression ratio achieved by this custom \"Chaos Codec\" and the resulting reconstruction error, comparing its efficiency to standard, general-purpose lossless compression tools."
      ],
      "metadata": {
        "id": "NDwKFdfSoO9g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpeclWVvno8p",
        "outputId": "5cbed0cc-9f4c-4385-cde0-459339fd4636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ Chaos Codec Accuracy:\n",
            " - MSE: 8.336860e-10\n",
            " - MAE: 0.000025\n",
            " - Cosine Similarity: 0.999999999\n",
            " - Loss Ratio (MSE / Var): 6.664851e-09\n",
            "\n",
            "ðŸ“¦ Compression Comparison (bytes + ratio):\n",
            " - Original Size      10485923 bytes | Ratio: 1.0000\n",
            " - Chaos Codec         2502381 bytes | Ratio: 0.2386\n",
            " - zlib                9899765 bytes | Ratio: 0.9441\n",
            " - gzip                9899777 bytes | Ratio: 0.9441\n",
            " - bz2                10162245 bytes | Ratio: 0.9691\n",
            " - lzma                9171812 bytes | Ratio: 0.8747\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle, zlib, gzip, bz2, lzma\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# === STEP 1: Generate large logistic signal (~10MB)\n",
        "def logistic_map(r, x0, n):\n",
        "    x = [x0]\n",
        "    for _ in range(n - 1):\n",
        "        x.append(r * x[-1] * (1 - x[-1]))\n",
        "    return np.array(x)\n",
        "\n",
        "n_large = 1310720  # 10MB in float64\n",
        "logistic_signal = logistic_map(r=4.0, x0=0.6, n=n_large)\n",
        "\n",
        "# === STEP 2: Train Henon-style base model\n",
        "X = np.vstack([\n",
        "    logistic_signal[:-2],\n",
        "    logistic_signal[1:-1],\n",
        "    logistic_signal[1:-1]**2\n",
        "]).T\n",
        "y = logistic_signal[2:]\n",
        "henon_model = LinearRegression().fit(X, y)\n",
        "W1, W2, W3 = henon_model.coef_\n",
        "b = henon_model.intercept_\n",
        "\n",
        "# === STEP 3: Predict base signal\n",
        "reconstructed_base = [logistic_signal[0], logistic_signal[1]]\n",
        "for _ in range(len(logistic_signal) - 2):\n",
        "    xn1, xn = reconstructed_base[-2], reconstructed_base[-1]\n",
        "    x2 = xn ** 2\n",
        "    reconstructed_base.append(W1 * xn1 + W2 * xn + W3 * x2 + b)\n",
        "reconstructed_base = np.array(reconstructed_base)\n",
        "\n",
        "# === STEP 4: Mirror-based linear residual\n",
        "residuals = logistic_signal - reconstructed_base\n",
        "X_mirror = np.vstack([\n",
        "    logistic_signal[:-2][::-1],\n",
        "    logistic_signal[1:-1][::-1]\n",
        "]).T\n",
        "y_resid = residuals[2:][::-1]\n",
        "residual_model = LinearRegression().fit(X_mirror, y_resid)\n",
        "R1, R2 = residual_model.coef_\n",
        "Rb = residual_model.intercept_\n",
        "\n",
        "# === STEP 5: Predict residuals and reconstruct hybrid\n",
        "resid_pred = []\n",
        "for i in range(len(logistic_signal) - 2):\n",
        "    m1 = logistic_signal[::-1][i]\n",
        "    m2 = logistic_signal[::-1][i+1]\n",
        "    resid_pred.append(R1 * m1 + R2 * m2 + Rb)\n",
        "resid_pred = [0, 0] + resid_pred[::-1]\n",
        "base_plus_mirror = reconstructed_base + np.array(resid_pred)\n",
        "\n",
        "# === STEP 6: Delta correction with int16 quantization\n",
        "delta = logistic_signal - base_plus_mirror\n",
        "scale = 1e4\n",
        "delta_q = np.clip(np.round(delta * scale), -32768, 32767).astype(np.int16)\n",
        "resid_payload = pickle.dumps({\"residuals\": delta_q, \"scale\": scale})\n",
        "resid_compressed = zlib.compress(resid_payload)\n",
        "\n",
        "# === STEP 7: Reconstruct final signal\n",
        "reconstructed_final = base_plus_mirror + (delta_q.astype(np.float32) / scale)\n",
        "\n",
        "# === STEP 8: Evaluate\n",
        "mse = mean_squared_error(logistic_signal, reconstructed_final)\n",
        "mae = mean_absolute_error(logistic_signal, reconstructed_final)\n",
        "cos_sim = cosine_similarity([logistic_signal], [reconstructed_final])[0, 0]\n",
        "loss_ratio = mse / np.var(logistic_signal)\n",
        "\n",
        "# === STEP 9: Compress weights (float16)\n",
        "weights = np.array([W1, W2, W3, b, R1, R2, Rb], dtype=np.float16)\n",
        "weights_compressed = zlib.compress(pickle.dumps(weights))\n",
        "\n",
        "# === STEP 10: Standard compressors\n",
        "raw_bytes = pickle.dumps(logistic_signal)\n",
        "zlib_bytes = zlib.compress(raw_bytes)\n",
        "gzip_bytes = gzip.compress(raw_bytes)\n",
        "bz2_bytes = bz2.compress(raw_bytes)\n",
        "lzma_bytes = lzma.compress(raw_bytes)\n",
        "\n",
        "# === STEP 11: Compare sizes and ratios\n",
        "sizes = {\n",
        "    \"Original Size\": len(raw_bytes),\n",
        "    \"Chaos Codec\": len(weights_compressed) + len(resid_compressed),\n",
        "    \"zlib\": len(zlib_bytes),\n",
        "    \"gzip\": len(gzip_bytes),\n",
        "    \"bz2\": len(bz2_bytes),\n",
        "    \"lzma\": len(lzma_bytes),\n",
        "}\n",
        "ratios = {k: round(v / sizes[\"Original Size\"], 4) for k, v in sizes.items()}\n",
        "\n",
        "# === Final Report ===\n",
        "print(\"ðŸŽ¯ Chaos Codec Accuracy:\")\n",
        "print(f\" - MSE: {mse:.6e}\")\n",
        "print(f\" - MAE: {mae:.6f}\")\n",
        "print(f\" - Cosine Similarity: {cos_sim:.9f}\")\n",
        "print(f\" - Loss Ratio (MSE / Var): {loss_ratio:.6e}\")\n",
        "print(\"\\nðŸ“¦ Compression Comparison (bytes + ratio):\")\n",
        "for name in sizes:\n",
        "    print(f\" - {name:<16} {sizes[name]:>10} bytes | Ratio: {ratios[name]:.4f}\")"
      ]
    }
  ]
}